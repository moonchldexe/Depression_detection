{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Uploading the dataset:"
      ],
      "metadata": {
        "id": "fa9OcLOb9T0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgE5nEQ7wqtw",
        "outputId": "b3111c0d-7a07-49c8-e202-e7963ba4d24c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vvtXTZg6xgfZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Depression detection /Datasets /Twitter dataset/clean_d_tweets.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Depression detection /Datasets /Twitter dataset/clean_non_d_tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A6T4XNrg1YOh"
      },
      "outputs": [],
      "source": [
        "df1['Labels']  = 1;\n",
        "df2['Labels'] = 0;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yPqe-H1S65yI"
      },
      "outputs": [],
      "source": [
        "df1 = df1[['tweet', 'Labels']]\n",
        "df2 = df2[['tweet', 'Labels']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tfh2tKvM7OfN"
      },
      "outputs": [],
      "source": [
        "merge = pd.concat([df1,df2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aiu0i-mQ7o9l"
      },
      "outputs": [],
      "source": [
        "merge = merge.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AARBbMkqxvLZ",
        "outputId": "114f0dc1-2ead-4b91-ca11-ec6ae7513749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "def remove_stop_words(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text)\n",
        "  filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "  return ' '.join(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BCdSQ86K8t9H"
      },
      "outputs": [],
      "source": [
        "merge['tweet'] = merge['tweet'].astype(str)\n",
        "merge['tweet'] = merge['tweet'].apply(remove_stop_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### here we are implementing Bert, tfidf, word2vec with different ML algorithms like : logistic regression, random forest, Support vector machine, Naives Bayes and RNN\n"
      ],
      "metadata": {
        "id": "zPg_rXmv-fJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Bert + ML algorithm"
      ],
      "metadata": {
        "id": "vQ7dT3G6oe-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(merge['tweet'], merge['Labels'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "itEzqn4_ojwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSuCdhk9pFku",
        "outputId": "94f412e8-e237-4554-b790-03bc92639ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert Implementation\n"
      ],
      "metadata": {
        "id": "wzG6CrMyGZEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def encode_texts_in_batches(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        encoded_inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "        embeddings.extend(batch_embeddings)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "X_train_embeddings = encode_texts_in_batches(X_train.tolist(), batch_size=16)\n",
        "X_test_embeddings = encode_texts_in_batches(X_test.tolist(), batch_size=16)\n"
      ],
      "metadata": {
        "id": "SzEqyxu-ojta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert + Logistic Regression"
      ],
      "metadata": {
        "id": "5kc7jywGGDiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train_embeddings, y_train)\n",
        "\n",
        "y_pred = lr_model.predict(X_test_embeddings)\n",
        "print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "hOGK8viqpeUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81a4bea-f2ed-4336-b953-d801e11b4c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.8468468468468469\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87       936\n",
            "           1       0.81      0.81      0.81       618\n",
            "\n",
            "    accuracy                           0.85      1554\n",
            "   macro avg       0.84      0.84      0.84      1554\n",
            "weighted avg       0.85      0.85      0.85      1554\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert + Random Forest\n"
      ],
      "metadata": {
        "id": "u1qFu3ebF5-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100)\n",
        "rf_model.fit(X_train_embeddings, y_train)\n",
        "y_pred = rf_model.predict(X_test_embeddings)\n",
        "\n",
        "print('Random Forest Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "U5rRtTE4ojqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222213a4-4bae-4f1c-b2ee-db71a3794a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.8326898326898327\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.87       936\n",
            "           1       0.82      0.74      0.78       618\n",
            "\n",
            "    accuracy                           0.83      1554\n",
            "   macro avg       0.83      0.82      0.82      1554\n",
            "weighted avg       0.83      0.83      0.83      1554\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert + Gaussian naives bayes"
      ],
      "metadata": {
        "id": "2iYGhCNOFnwx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siXie79X8t50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d1c8f8f-6933-4ec0-9a67-766edb4736d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7477\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.74      0.78       936\n",
            "           1       0.66      0.76      0.71       618\n",
            "\n",
            "    accuracy                           0.75      1554\n",
            "   macro avg       0.74      0.75      0.74      1554\n",
            "weighted avg       0.76      0.75      0.75      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train_embeddings, y_train)\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test_embeddings)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert + support vector"
      ],
      "metadata": {
        "id": "XtQtZqGEEDdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "svm_classifier.fit(X_train_embeddings, y_train)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test_embeddings)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e09LnhXp5EjX",
        "outputId": "dfcd0e2c-9ed6-4272-fd0d-7e99f6546465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8449\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87       936\n",
            "           1       0.79      0.83      0.81       618\n",
            "\n",
            "    accuracy                           0.84      1554\n",
            "   macro avg       0.84      0.84      0.84      1554\n",
            "weighted avg       0.85      0.84      0.85      1554\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert + RNN"
      ],
      "metadata": {
        "id": "4uT3RDWNDi_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embeddings(texts, max_length=32):\n",
        "    encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=max_length)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_inputs)\n",
        "    embeddings = outputs.last_hidden_state\n",
        "    return embeddings\n",
        "\n",
        "texts = merge['tweet'].tolist()\n",
        "\n",
        "embeddings = get_bert_embeddings(texts)\n"
      ],
      "metadata": {
        "id": "jdlXXhGL7X6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "input_size = 768\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "\n",
        "rnn_model = RNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "labels =  merge['Labels']\n",
        "\n",
        "embeddings = embeddings.squeeze(0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=2, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "sbsqNxvb6Ruv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "rnn_model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (texts, labels) in enumerate(train_loader):\n",
        "\n",
        "        outputs = rnn_model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "FLLXFqwcIHuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9420a34a-ac82-4514-960d-41a551545502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0516\n",
            "Epoch [2/5], Loss: 0.0437\n",
            "Epoch [3/5], Loss: 0.0075\n",
            "Epoch [4/5], Loss: 0.0417\n",
            "Epoch [5/5], Loss: 0.1289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "rnn_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        outputs = rnn_model(texts)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "wGeVkR0VEZlI",
        "outputId": "4ce23a51-0a88-4d6c-c292-4ed00667837c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF and ML algorithm"
      ],
      "metadata": {
        "id": "ziYJsjcrnIXf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aKclD6rzjPN",
        "outputId": "2b323f81-7906-4ce1-e600-aa7b423fb631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8365508365508365\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.95      0.87       930\n",
            "           1       0.89      0.67      0.77       624\n",
            "\n",
            "    accuracy                           0.84      1554\n",
            "   macro avg       0.85      0.81      0.82      1554\n",
            "weighted avg       0.84      0.84      0.83      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words = 'english')\n",
        "x = tfidf.fit_transform(merge['tweet'])\n",
        "\n",
        "\n",
        "\n",
        "#1.1 TF-IDF + Logistic Regression\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, merge['Labels'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"classification report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-idf + Random forest"
      ],
      "metadata": {
        "id": "SXsy82BHV5V4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-qu_Lc7y44f",
        "outputId": "ea9667c8-4d2f-4371-be8f-dd94bdd7f30c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8223938223938224\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84       918\n",
            "           1       0.75      0.84      0.79       636\n",
            "\n",
            "    accuracy                           0.82      1554\n",
            "   macro avg       0.82      0.83      0.82      1554\n",
            "weighted avg       0.83      0.82      0.82      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_rf = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
        "model_rf.fit(x_train,y_train)\n",
        "\n",
        "y_pred = model_rf.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-idf + Support vector machine"
      ],
      "metadata": {
        "id": "jv4u5eLWWhsz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV_FA25-y4ys",
        "outputId": "ffe2e247-9e80-4e3e-ee1e-a288d7a35a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8449163449163449\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87       918\n",
            "           1       0.84      0.76      0.80       636\n",
            "\n",
            "    accuracy                           0.84      1554\n",
            "   macro avg       0.84      0.83      0.84      1554\n",
            "weighted avg       0.84      0.84      0.84      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model_sv = SVC(kernel='linear', random_state=42)\n",
        "model_sv.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model_sv.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-idf + Multinomial naives bayes\n"
      ],
      "metadata": {
        "id": "JYJ2sEqdY2jS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sSo7L3kz7zs",
        "outputId": "0e280530-125a-48cd-cd2f-6604ffd2c544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8584298584298584\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.92      0.89       935\n",
            "           1       0.86      0.77      0.81       619\n",
            "\n",
            "    accuracy                           0.86      1554\n",
            "   macro avg       0.86      0.84      0.85      1554\n",
            "weighted avg       0.86      0.86      0.86      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(x_train, y_train)\n",
        "\n",
        "y_pred = model_nb.predict(x_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{report}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-idf + RNN"
      ],
      "metadata": {
        "id": "iKGA2qypaf-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "LVAm1gWhz7wZ",
        "outputId": "2fed3b35-e63a-4a95-b83f-673af40626ad"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │           \u001b[38;5;34m2,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,651</span> (10.36 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,651\u001b[0m (10.36 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,651</span> (10.36 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,651\u001b[0m (10.36 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 1s/step - accuracy: 0.6027 - loss: 0.6780 - val_accuracy: 0.6299 - val_loss: 0.6650\n",
            "Epoch 2/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 2s/step - accuracy: 0.5935 - loss: 0.6769 - val_accuracy: 0.6299 - val_loss: 0.6649\n",
            "Epoch 3/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 2s/step - accuracy: 0.6050 - loss: 0.6732 - val_accuracy: 0.6299 - val_loss: 0.6634\n",
            "Epoch 4/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 2s/step - accuracy: 0.5950 - loss: 0.6765 - val_accuracy: 0.6299 - val_loss: 0.6633\n",
            "Epoch 5/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 2s/step - accuracy: 0.5884 - loss: 0.6794 - val_accuracy: 0.6299 - val_loss: 0.6611\n",
            "Epoch 6/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 2s/step - accuracy: 0.6061 - loss: 0.6710 - val_accuracy: 0.6299 - val_loss: 0.6622\n",
            "Epoch 7/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 2s/step - accuracy: 0.5937 - loss: 0.6761 - val_accuracy: 0.6299 - val_loss: 0.6607\n",
            "Epoch 8/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 2s/step - accuracy: 0.5984 - loss: 0.6736 - val_accuracy: 0.6299 - val_loss: 0.6626\n",
            "Epoch 9/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 2s/step - accuracy: 0.5945 - loss: 0.6753 - val_accuracy: 0.6299 - val_loss: 0.6640\n",
            "Epoch 10/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 2s/step - accuracy: 0.6006 - loss: 0.6734 - val_accuracy: 0.6299 - val_loss: 0.6603\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 439ms/step\n",
            "Accuracy: 0.5984555984555985\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75       930\n",
            "           1       0.00      0.00      0.00       624\n",
            "\n",
            "    accuracy                           0.60      1554\n",
            "   macro avg       0.30      0.50      0.37      1554\n",
            "weighted avg       0.36      0.60      0.45      1554\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "merge['Labels'] = label_encoder.fit_transform(merge['Labels'])\n",
        "\n",
        "X = tfidf.fit_transform(merge['tweet']).toarray()\n",
        "y = merge['Labels'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, input_shape=(X_train.shape[1], 1), activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Classification Report:\\n{report}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD2VEC + ML algorithms\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kf9cQlhtJe5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SE0MWA1Kz7uX"
      },
      "outputs": [],
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tokenized_text = merge['tweet'].apply(word_tokenize)\n",
        "\n",
        "model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "    nwords = 0.\n",
        "\n",
        "    for word in words:\n",
        "        if word in vocabulary:\n",
        "            nwords = nwords + 1.\n",
        "            feature_vector = np.add(feature_vector, model.wv[word])\n",
        "\n",
        "    if nwords:\n",
        "        feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "# Function to generate averaged word vector features for the training and test data\n",
        "def averaged_word_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index_to_key)\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)\n",
        "\n",
        "# Get averaged word vectors for the text data\n",
        "wordvec_arrays = averaged_word_vectorizer(tokenized_text, model=model, num_features=100)\n",
        "X = pd.DataFrame(wordvec_arrays)\n",
        "y = np.array(merge['Labels'])  # Replace with your actual labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word2vec + Logistic regression"
      ],
      "metadata": {
        "id": "IJsamb6NbCHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iY3UTzJz7sV",
        "outputId": "8443bb56-9403-44b8-e927-6b0659686cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.71\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.89      0.79       948\n",
            "           1       0.71      0.42      0.53       606\n",
            "\n",
            "    accuracy                           0.71      1554\n",
            "   macro avg       0.71      0.65      0.66      1554\n",
            "weighted avg       0.71      0.71      0.69      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test,y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print(f'Classification report:\\n{report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2vec + random forest\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vwrI1poJbiUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvqPDI_Sz7qG",
        "outputId": "4fa23e2d-2170-4023-be09-173497170af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.7638352638352638\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.86      0.81       935\n",
            "           1       0.74      0.62      0.68       619\n",
            "\n",
            "    accuracy                           0.76      1554\n",
            "   macro avg       0.76      0.74      0.75      1554\n",
            "weighted avg       0.76      0.76      0.76      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {accuracy}\")\n",
        "print(f\"Random Forest Classification Report:\\n{report}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WORD2Vec + Support vector machine"
      ],
      "metadata": {
        "id": "yNg6uHpMbr_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5vUSKhuz7oI",
        "outputId": "d2bcdb99-e08b-4e90-896d-d53afdbdc5c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM classification accuracy: 0.7123552123552124\n",
            "SVM classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.94      0.80       935\n",
            "           1       0.80      0.37      0.51       619\n",
            "\n",
            "    accuracy                           0.71      1554\n",
            "   macro avg       0.74      0.66      0.65      1554\n",
            "weighted avg       0.73      0.71      0.68      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"SVM classification accuracy: {accuracy}\")\n",
        "print(f\"SVM classification report:\\n{report}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2vec + Gaussian Naive bayes"
      ],
      "metadata": {
        "id": "uwSKckLtcTD8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfbl1Patz7lo",
        "outputId": "34d59273-98c3-49b4-8ddb-59385bbc91cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.6512226512226512\n",
            "Gaussian Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.74      0.72       948\n",
            "           1       0.56      0.52      0.54       606\n",
            "\n",
            "    accuracy                           0.65      1554\n",
            "   macro avg       0.63      0.63      0.63      1554\n",
            "weighted avg       0.65      0.65      0.65      1554\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Gaussian Naive Bayes Accuracy: {accuracy}\")\n",
        "print(f\"Gaussian Naive Bayes Classification Report:\\n{report}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vedc + RNN"
      ],
      "metadata": {
        "id": "OnyedWPGcpfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9YWOIV9UC6x",
        "outputId": "da316e51-30e2-495d-c7f2-5c9f0c8df85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "merge['Labels'] = label_encoder.fit_transform(merge['Labels'])\n",
        "\n",
        "w2v_model = Word2Vec(sentences=merge['tweet'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "word_index = {word: index + 1 for index, word in enumerate(w2v_model.wv.index_to_key)}\n",
        "sequences = [[word_index[word] for word in text if word in word_index] for text in merge['tweet']]\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "y = to_categorical(merge['Labels'])\n",
        "\n",
        "embedding_dim = w2v_model.vector_size\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in w2v_model.wv:\n",
        "        embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdbE4JBKUC3-",
        "outputId": "4ccb844c-f71c-406f-f055-d3b4118f6821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - accuracy: 0.5709 - loss: 0.6710 - val_accuracy: 0.6315 - val_loss: 0.6471\n",
            "Epoch 2/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 86ms/step - accuracy: 0.6253 - loss: 17.9255 - val_accuracy: 0.6211 - val_loss: 0.6450\n",
            "Epoch 3/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 92ms/step - accuracy: 0.6134 - loss: 0.6442 - val_accuracy: 0.6098 - val_loss: 0.6366\n",
            "Epoch 4/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.6587 - loss: 0.6117 - val_accuracy: 0.6758 - val_loss: 0.5967\n",
            "Epoch 5/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 86ms/step - accuracy: 0.7074 - loss: 0.5750 - val_accuracy: 0.7530 - val_loss: 0.5127\n",
            "Epoch 6/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 86ms/step - accuracy: 0.7499 - loss: 0.5201 - val_accuracy: 0.6983 - val_loss: 0.5506\n",
            "Epoch 7/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 77ms/step - accuracy: 0.7542 - loss: 0.5095 - val_accuracy: 0.7498 - val_loss: 0.5258\n",
            "Epoch 8/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.7801 - loss: 0.4719 - val_accuracy: 0.7611 - val_loss: 0.5005\n",
            "Epoch 9/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 90ms/step - accuracy: 0.7988 - loss: 0.4497 - val_accuracy: 0.7377 - val_loss: 0.5315\n",
            "Epoch 10/10\n",
            "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 88ms/step - accuracy: 0.6847 - loss: 0.5788 - val_accuracy: 0.7345 - val_loss: 0.5002\n",
            "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step - accuracy: 0.7580 - loss: 0.5091\n",
            "RNN Accuracy: 0.7548262476921082\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
        "model.add(SimpleRNN(100, activation='relu'))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"RNN Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## from here we are starting on the Hypertuning SVM parameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MYdgKtUAEbfP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s921kPJERED"
      },
      "source": [
        "## BEE COLONY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-iKBoDYEUG_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(merge['tweet'], merge['Labels'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mpjP2o9EUEL"
      },
      "outputs": [],
      "source": [
        "class ArtificialBeeColony:\n",
        "    def __init__(self, obj_func, bounds, colony_size, max_iter, limit):\n",
        "        self.obj_func = obj_func\n",
        "        self.bounds = bounds\n",
        "        self.colony_size = colony_size\n",
        "        self.max_iter = max_iter\n",
        "        self.limit = limit\n",
        "        self.dim = len(bounds)\n",
        "\n",
        "        self.food_sources = np.random.rand(colony_size, self.dim)\n",
        "        self.food_sources = self.bounds[:, 0] + self.food_sources * (self.bounds[:, 1] - self.bounds[:, 0])\n",
        "        self.fitness = np.array([self.obj_func(ind) for ind in self.food_sources])\n",
        "        self.trial = np.zeros(colony_size)\n",
        "\n",
        "    def optimize(self):\n",
        "        for iter in range(self.max_iter):\n",
        "            # Employed bee phase\n",
        "            for i in range(self.colony_size):\n",
        "                k = np.random.randint(0, self.colony_size)\n",
        "                while k == i:\n",
        "                    k = np.random.randint(0, self.colony_size)\n",
        "\n",
        "                phi = np.random.uniform(-1, 1, self.dim)\n",
        "                new_solution = self.food_sources[i] + phi * (self.food_sources[i] - self.food_sources[k])\n",
        "                new_solution = np.clip(new_solution, self.bounds[:, 0], self.bounds[:, 1])\n",
        "\n",
        "                new_fitness = self.obj_func(new_solution)\n",
        "\n",
        "                if new_fitness < self.fitness[i]:\n",
        "                    self.food_sources[i] = new_solution\n",
        "                    self.fitness[i] = new_fitness\n",
        "                    self.trial[i] = 0\n",
        "                else:\n",
        "                    self.trial[i] += 1\n",
        "\n",
        "            # Onlooker bee phase\n",
        "            fitness_prob = self.fitness / np.sum(self.fitness)\n",
        "            for i in range(self.colony_size):\n",
        "                r = np.random.rand()\n",
        "                if r < fitness_prob[i]:\n",
        "                    k = np.random.randint(0, self.colony_size)\n",
        "                    while k == i:\n",
        "                        k = np.random.randint(0, self.colony_size)\n",
        "\n",
        "                    phi = np.random.uniform(-1, 1, self.dim)\n",
        "                    new_solution = self.food_sources[i] + phi * (self.food_sources[i] - self.food_sources[k])\n",
        "                    new_solution = np.clip(new_solution, self.bounds[:, 0], self.bounds[:, 1])\n",
        "\n",
        "                    new_fitness = self.obj_func(new_solution)\n",
        "\n",
        "                    if new_fitness < self.fitness[i]:\n",
        "                        self.food_sources[i] = new_solution\n",
        "                        self.fitness[i] = new_fitness\n",
        "                        self.trial[i] = 0\n",
        "                    else:\n",
        "                        self.trial[i] += 1\n",
        "\n",
        "            # Scout bee phase\n",
        "            for i in range(self.colony_size):\n",
        "                if self.trial[i] > self.limit:\n",
        "                    self.food_sources[i] = self.bounds[:, 0] + np.random.rand(self.dim) * (self.bounds[:, 1] - self.bounds[:, 0])\n",
        "                    self.fitness[i] = self.obj_func(self.food_sources[i])\n",
        "                    self.trial[i] = 0\n",
        "\n",
        "            best_index = np.argmin(self.fitness)\n",
        "            print(f\"Iteration {iter+1}/{self.max_iter}, Best Fitness: {self.fitness[best_index]}\")\n",
        "\n",
        "        best_index = np.argmin(self.fitness)\n",
        "        return self.food_sources[best_index], self.fitness[best_index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvA79xqdEUBD"
      },
      "outputs": [],
      "source": [
        "def svm_objective(params):\n",
        "    C, gamma = params\n",
        "    C = max(C, 0.1)  # Ensure C is within the valid range\n",
        "    gamma = max(gamma, 0.0001)  # Ensure gamma is within the valid range\n",
        "    svm = SVC(C=C, gamma=gamma)\n",
        "    svm.fit(X_train_tfidf, y_train)\n",
        "    y_pred = svm.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return -accuracy  # Minimize negative accuracy to maximize accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEPg61r2E93U",
        "outputId": "39675406-996f-4ddb-c426-930892fe5af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 2/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 3/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 4/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 5/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 6/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 7/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 8/50, Best Fitness: -0.8674388674388674\n",
            "Iteration 9/50, Best Fitness: -0.8687258687258688\n",
            "Iteration 10/50, Best Fitness: -0.8687258687258688\n",
            "Iteration 11/50, Best Fitness: -0.8687258687258688\n",
            "Iteration 12/50, Best Fitness: -0.8687258687258688\n",
            "Iteration 13/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 14/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 15/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 16/50, Best Fitness: -0.87001287001287\n",
            "Iteration 17/50, Best Fitness: -0.87001287001287\n",
            "Iteration 18/50, Best Fitness: -0.87001287001287\n",
            "Iteration 19/50, Best Fitness: -0.87001287001287\n",
            "Iteration 20/50, Best Fitness: -0.87001287001287\n",
            "Iteration 21/50, Best Fitness: -0.87001287001287\n",
            "Iteration 22/50, Best Fitness: -0.87001287001287\n",
            "Iteration 23/50, Best Fitness: -0.87001287001287\n",
            "Iteration 24/50, Best Fitness: -0.87001287001287\n",
            "Iteration 25/50, Best Fitness: -0.87001287001287\n",
            "Iteration 26/50, Best Fitness: -0.87001287001287\n",
            "Iteration 27/50, Best Fitness: -0.87001287001287\n",
            "Iteration 28/50, Best Fitness: -0.87001287001287\n",
            "Iteration 29/50, Best Fitness: -0.87001287001287\n",
            "Iteration 30/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 31/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 32/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 33/50, Best Fitness: -0.87001287001287\n",
            "Iteration 34/50, Best Fitness: -0.87001287001287\n",
            "Iteration 35/50, Best Fitness: -0.87001287001287\n",
            "Iteration 36/50, Best Fitness: -0.87001287001287\n",
            "Iteration 37/50, Best Fitness: -0.87001287001287\n",
            "Iteration 38/50, Best Fitness: -0.87001287001287\n",
            "Iteration 39/50, Best Fitness: -0.87001287001287\n",
            "Iteration 40/50, Best Fitness: -0.87001287001287\n",
            "Iteration 41/50, Best Fitness: -0.87001287001287\n",
            "Iteration 42/50, Best Fitness: -0.87001287001287\n",
            "Iteration 43/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 44/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 45/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 46/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 47/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 48/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 49/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 50/50, Best Fitness: -0.8693693693693694\n",
            "Best Parameters: C=8.05519963357078, gamma=0.48341943659360936\n",
            "Best Accuracy: 0.8693693693693694\n"
          ]
        }
      ],
      "source": [
        "# Define parameter bounds (C and gamma)\n",
        "bounds = np.array([[0.1, 100], [0.0001, 1]])\n",
        "\n",
        "# Artificial Bee Colony parameters\n",
        "colony_size = 20\n",
        "max_iter = 50\n",
        "limit = 10\n",
        "\n",
        "# Run Artificial Bee Colony algorithm\n",
        "abc = ArtificialBeeColony(svm_objective, bounds, colony_size, max_iter, limit)\n",
        "best_params, best_fitness = abc.optimize()\n",
        "\n",
        "# Output best parameters\n",
        "print(f\"Best Parameters: C={best_params[0]}, gamma={best_params[1]}\")\n",
        "print(f\"Best Accuracy: {-best_fitness}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwdMduJmE90N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Sample data\n",
        "data = fetch_20newsgroups(subset='train')\n",
        "df = pd.DataFrame({'text': data.data, 'label': data.target})\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDtUAgxtE9yF"
      },
      "outputs": [],
      "source": [
        "def levy_flight(Lambda):\n",
        "    sigma1 = np.power((np.math.gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2)) / np.math.gamma((1 + Lambda) / 2) * Lambda * np.power(2, (Lambda - 1) / 2), 1 / Lambda)\n",
        "    sigma2 = 1\n",
        "    u = np.random.normal(0, sigma1, 1)\n",
        "    v = np.random.normal(0, sigma2, 1)\n",
        "    step = u / np.power(np.abs(v), 1 / Lambda)\n",
        "    return step\n",
        "\n",
        "def objective_function(params):\n",
        "    C, gamma = params\n",
        "    svm = SVC(C=C, gamma=gamma)\n",
        "    svm.fit(X_train_tfidf, y_train)\n",
        "    y_pred = svm.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return -accuracy  # Negative because we need to minimize the objective function\n",
        "\n",
        "def simple_bounds(s, lb, ub):\n",
        "    ns_temp = s\n",
        "    ns_temp[s < lb] = lb[s < lb]\n",
        "    ns_temp[s > ub] = ub[s > ub]\n",
        "    return ns_temp\n",
        "\n",
        "def cuckoo_search(n=25, pa=0.25, Lambda=1.5, num_iter=50):\n",
        "    dim = 2\n",
        "    lb = [0.01, 0.0001]\n",
        "    ub = [100, 1]\n",
        "    nest = np.random.rand(n, dim) * (ub - lb) + lb\n",
        "    fitness = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        fitness[i] = objective_function(nest[i, :])\n",
        "\n",
        "    best_nest = nest[np.argmin(fitness), :]\n",
        "    best_fitness = np.min(fitness)\n",
        "\n",
        "    for iter in range(num_iter):\n",
        "        new_nest = np.zeros((n, dim))\n",
        "        for i in range(n):\n",
        "            step_size = levy_flight(Lambda)\n",
        "            step = step_size * (nest[i, :] - best_nest)\n",
        "            new_nest[i, :] = nest[i, :] + step * np.random.randn(dim)\n",
        "            new_nest[i, :] = simple_bounds(new_nest[i, :], lb, ub)\n",
        "\n",
        "        for i in range(n):\n",
        "            fnew = objective_function(new_nest[i, :])\n",
        "            if fnew < fitness[i]:\n",
        "                fitness[i] = fnew\n",
        "                nest[i, :] = new_nest[i, :]\n",
        "\n",
        "        indices = np.argsort(fitness)\n",
        "        n_best = int(n * pa)\n",
        "        nest[indices[:n_best], :] = best_nest\n",
        "\n",
        "        best_nest = nest[np.argmin(fitness), :]\n",
        "        best_fitness = np.min(fitness)\n",
        "\n",
        "        print(f'Iteration {iter+1}/{num_iter}, Best Fitness: {-best_fitness}')\n",
        "\n",
        "    return best_nest, -best_fitness\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_accuracy = cuckoo_search(n=25, pa=0.25, Lambda=1.5, num_iter=50)\n",
        "print(f'Best Parameters: C={best_params[0]}, gamma={best_params[1]}')\n",
        "print(f'Best Accuracy: {best_accuracy}')\n"
      ],
      "metadata": {
        "id": "ODtoRmYhsUD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PSO"
      ],
      "metadata": {
        "id": "g8TWgcRHI_Cz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb2B8XDfwPXN",
        "outputId": "d9cb6c9b-d89a-42af-dd74-2ef8ae5821a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting pyswarm\n",
            "  Downloading pyswarm-0.6.tar.gz (4.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Building wheels for collected packages: pyswarm\n",
            "  Building wheel for pyswarm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyswarm: filename=pyswarm-0.6-py3-none-any.whl size=4464 sha256=6494a009f250490cdb4b4dbddaa46002c79c533913ee9ef021b75c620dfebb98\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/40/62fa158f497f942277cbab8199b05cb61c571ab324e67ad0d6\n",
            "Successfully built pyswarm\n",
            "Installing collected packages: pyswarm\n",
            "Successfully installed pyswarm-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn pyswarm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8IbY74awTnZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Extract the text and label columns\n",
        "texts = merge['tweet'].astype(str).tolist()\n",
        "labels = merge['Labels'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(texts_train)\n",
        "X_test = vectorizer.transform(texts_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lYuwGFqwV3Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pyswarm import pso\n",
        "\n",
        "def evaluate_individual(params):\n",
        "    C, gamma = params\n",
        "\n",
        "    model = SVC(C=C, gamma=gamma, kernel='rbf')\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    model.fit(X_train, labels_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(labels_test, y_pred)\n",
        "\n",
        "    return -accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "MyzKOKBHx1Um",
        "outputId": "71f0dd8e-0047-444f-e24e-a69d5f59f9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stopping search: maximum iterations reached --> 20\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=894753.2096413812, gamma=1.1348094395599146)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=894753.2096413812, gamma=1.1348094395599146)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC(C=894753.2096413812, gamma=1.1348094395599146)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lb = [1e-6, 1e-6]\n",
        "ub = [1e+6, 1e+1]\n",
        "\n",
        "best_params, _ = pso(evaluate_individual, lb, ub, swarmsize=10, maxiter=20)\n",
        "\n",
        "best_C, best_gamma = best_params\n",
        "\n",
        "best_model = SVC(C=best_C, gamma=best_gamma, kernel='rbf')\n",
        "best_model.fit(X_train, labels_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY1yK0253cCD",
        "outputId": "178999c2-9a9b-4327-b273-25a01bc41bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: C = 970705.2288605786, gamma = 1.5876338750856334\n",
            "Test Set Accuracy: 87.64%\n"
          ]
        }
      ],
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(labels_test, y_pred)\n",
        "print(f\"Best Parameters: C = {970705.2288605786}, gamma = {1.5876338750856334}\")\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJaf_qIotBu0"
      },
      "source": [
        "## FLOWER POLLINATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A37ut-884k4",
        "outputId": "0b8615c5-04f6-4e71-cd6e-f6c8839d5223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ToWaXiJ84id"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Extract the text and label columns\n",
        "texts = merge['tweet'].astype(str).tolist()\n",
        "labels = merge['Labels'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(texts_train)\n",
        "X_test = vectorizer.transform(texts_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik8Fa6aW84cb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the Flower Pollination Algorithm (FPA) function\n",
        "def flower_pollination_algorithm(fitness_func, lb, ub, num_flowers, max_gen):\n",
        "    \"\"\"\n",
        "    FPA implementation for optimizing SVM hyperparameters.\n",
        "    Parameters:\n",
        "    - fitness_func: Function to evaluate fitness (accuracy) of SVM with given hyperparameters\n",
        "    - lb: Lower bounds for hyperparameters (C, gamma)\n",
        "    - ub: Upper bounds for hyperparameters (C, gamma)\n",
        "    - num_flowers: Number of flowers (population size)\n",
        "    - max_gen: Maximum number of generations (iterations)\n",
        "\n",
        "    Returns:\n",
        "    - best_params: Best hyperparameters found by FPA\n",
        "    \"\"\"\n",
        "    flowers = np.random.uniform(lb, ub, size=(num_flowers, len(lb)))\n",
        "    best_flower = None\n",
        "    best_fitness = -np.inf\n",
        "\n",
        "    for gen in range(max_gen):\n",
        "\n",
        "        fitness = np.array([fitness_func(flower) for flower in flowers])\n",
        "\n",
        "        max_index = np.argmax(fitness)\n",
        "        if fitness[max_index] > best_fitness:\n",
        "            best_flower = flowers[max_index]\n",
        "            best_fitness = fitness[max_index]\n",
        "\n",
        "        for i in range(num_flowers):\n",
        "            beta = np.random.uniform(-1, 1, size=len(lb))\n",
        "            step_size = np.random.uniform(0, 1)\n",
        "            new_flower = flowers[i] + step_size * beta * (flowers[max_index] - flowers[i])\n",
        "\n",
        "            new_flower = np.clip(new_flower, lb, ub)\n",
        "            if fitness_func(new_flower) > fitness[i]:\n",
        "                flowers[i] = new_flower\n",
        "\n",
        "    return best_flower\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFg_8Q-R6B_8",
        "outputId": "125997ef-7b5e-49f0-f026-33e51933e6f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: C = 958859.9790305122, gamma = 1.3730403661081159\n",
            "Test Set Accuracy: 86.23%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def evaluate_individual(params):\n",
        "    C, gamma = params\n",
        "    model = SVC(C=C, gamma=gamma, kernel='rbf')\n",
        "\n",
        "    model.fit(X_train, labels_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(labels_test, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "lb = [1e-6, 1e-6]\n",
        "ub = [1e+6, 1e+1]\n",
        "\n",
        "best_params = flower_pollination_algorithm(evaluate_individual, lb, ub, num_flowers=20, max_gen=50)\n",
        "best_C, best_gamma = best_params\n",
        "\n",
        "best_model = SVC(C=best_C, gamma=best_gamma, kernel='rbf')\n",
        "best_model.fit(X_train, labels_train)\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(labels_test, y_pred)\n",
        "print(f\"Best Parameters: C = {best_C}, gamma = {best_gamma}\")\n",
        "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euafs8xhA_WW"
      },
      "source": [
        "## SMRO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEaKOazuuAXJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(merge['tweet'], merge['Labels'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Text vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZNdhkIRt8Rk"
      },
      "outputs": [],
      "source": [
        "def smro_algorithm(obj_func, bounds, population_size, max_iter):\n",
        "    # Initialize the population\n",
        "    population = np.random.rand(population_size, len(bounds))\n",
        "    population = bounds[:, 0] + population * (bounds[:, 1] - bounds[:, 0])\n",
        "    fitness = np.zeros(population_size)\n",
        "\n",
        "    # Evaluate initial population\n",
        "    for i in range(population_size):\n",
        "        fitness[i] = obj_func(population[i])\n",
        "\n",
        "    # Iterate to evolve solutions\n",
        "    for iteration in range(max_iter):\n",
        "        for i in range(population_size):\n",
        "\n",
        "            candidate = population[i] + np.random.uniform(-1, 1, len(bounds)) * (population[np.random.randint(population_size)] - population[i])\n",
        "            candidate = np.clip(candidate, bounds[:, 0], bounds[:, 1])\n",
        "            candidate_fitness = obj_func(candidate)\n",
        "\n",
        "            if candidate_fitness < fitness[i]:\n",
        "                population[i] = candidate\n",
        "                fitness[i] = candidate_fitness\n",
        "\n",
        "        print(f\"Iteration {iteration+1}/{max_iter}, Best Fitness: {min(fitness)}\")\n",
        "\n",
        "    best_idx = np.argmin(fitness)\n",
        "    return population[best_idx], fitness[best_idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltHXdLHUuA2B"
      },
      "outputs": [],
      "source": [
        "def svm_objective(params):\n",
        "    C, gamma = params\n",
        "    svm = SVC(C=C, gamma=gamma)\n",
        "    svm.fit(X_train_tfidf, y_train)\n",
        "    y_pred = svm.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return -accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBNk3zbuyAE1",
        "outputId": "2595e584-3446-4e91-d81c-9a71f52cc569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/50, Best Fitness: -0.868082368082368\n",
            "Iteration 2/50, Best Fitness: -0.868082368082368\n",
            "Iteration 3/50, Best Fitness: -0.868082368082368\n",
            "Iteration 4/50, Best Fitness: -0.868082368082368\n",
            "Iteration 5/50, Best Fitness: -0.868082368082368\n",
            "Iteration 6/50, Best Fitness: -0.868082368082368\n",
            "Iteration 7/50, Best Fitness: -0.8687258687258688\n",
            "Iteration 8/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 9/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 10/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 11/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 12/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 13/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 14/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 15/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 16/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 17/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 18/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 19/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 20/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 21/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 22/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 23/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 24/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 25/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 26/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 27/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 28/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 29/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 30/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 31/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 32/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 33/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 34/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 35/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 36/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 37/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 38/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 39/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 40/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 41/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 42/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 43/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 44/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 45/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 46/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 47/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 48/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 49/50, Best Fitness: -0.8693693693693694\n",
            "Iteration 50/50, Best Fitness: -0.8693693693693694\n",
            "Best Parameters: C=1.686859970878075, gamma=1.0\n",
            "Best Accuracy: 0.8693693693693694\n"
          ]
        }
      ],
      "source": [
        "\n",
        "bounds = np.array([[0.1, 100], [0.0001, 1]])\n",
        "\n",
        "population_size = 10\n",
        "max_iter = 50\n",
        "best_params, best_fitness = smro_algorithm(svm_objective, bounds, population_size, max_iter)\n",
        "\n",
        "print(f\"Best Parameters: C={best_params[0]}, gamma={best_params[1]}\")\n",
        "print(f\"Best Accuracy: {-best_fitness}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuZe3rn1A271"
      },
      "source": [
        "## GREY WOLF OPTIMIZER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHQQHRguASN0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(merge['tweet'], merge['Labels'], test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y8xLOE6AR-4"
      },
      "outputs": [],
      "source": [
        "class GreyWolfOptimizer:\n",
        "    def __init__(self, obj_func, bounds, population_size, max_iter):\n",
        "        self.obj_func = obj_func\n",
        "        self.bounds = bounds\n",
        "        self.population_size = population_size\n",
        "        self.max_iter = max_iter\n",
        "        self.dim = len(bounds)\n",
        "        self.alpha_pos = np.zeros(self.dim)\n",
        "        self.alpha_score = float(\"inf\")\n",
        "        self.beta_pos = np.zeros(self.dim)\n",
        "        self.beta_score = float(\"inf\")\n",
        "        self.delta_pos = np.zeros(self.dim)\n",
        "        self.delta_score = float(\"inf\")\n",
        "\n",
        "    def optimize(self):\n",
        "        # Initialize the positions of wolves\n",
        "        population = np.random.rand(self.population_size, self.dim)\n",
        "        population = self.bounds[:, 0] + population * (self.bounds[:, 1] - self.bounds[:, 0])\n",
        "\n",
        "        for iter in range(self.max_iter):\n",
        "            for i in range(self.population_size):\n",
        "                fitness = self.obj_func(population[i])\n",
        "\n",
        "                # Update alpha, beta, and delta wolves\n",
        "                if fitness < self.alpha_score:\n",
        "                    self.delta_score = self.beta_score\n",
        "                    self.delta_pos = self.beta_pos.copy()\n",
        "                    self.beta_score = self.alpha_score\n",
        "                    self.beta_pos = self.alpha_pos.copy()\n",
        "                    self.alpha_score = fitness\n",
        "                    self.alpha_pos = population[i].copy()\n",
        "                elif fitness < self.beta_score:\n",
        "                    self.delta_score = self.beta_score\n",
        "                    self.delta_pos = self.beta_pos.copy()\n",
        "                    self.beta_score = fitness\n",
        "                    self.beta_pos = population[i].copy()\n",
        "                elif fitness < self.delta_score:\n",
        "                    self.delta_score = fitness\n",
        "                    self.delta_pos = population[i].copy()\n",
        "\n",
        "            a = 2 - iter * (2 / self.max_iter)  # Decrease linearly from 2 to 0\n",
        "\n",
        "            for i in range(self.population_size):\n",
        "                for j in range(self.dim):\n",
        "                    r1 = np.random.rand()\n",
        "                    r2 = np.random.rand()\n",
        "\n",
        "                    A1 = 2 * a * r1 - a\n",
        "                    C1 = 2 * r2\n",
        "                    D_alpha = abs(C1 * self.alpha_pos[j] - population[i, j])\n",
        "                    X1 = self.alpha_pos[j] - A1 * D_alpha\n",
        "\n",
        "                    r1 = np.random.rand()\n",
        "                    r2 = np.random.rand()\n",
        "\n",
        "                    A2 = 2 * a * r1 - a\n",
        "                    C2 = 2 * r2\n",
        "                    D_beta = abs(C2 * self.beta_pos[j] - population[i, j])\n",
        "                    X2 = self.beta_pos[j] - A2 * D_beta\n",
        "\n",
        "                    r1 = np.random.rand()\n",
        "                    r2 = np.random.rand()\n",
        "\n",
        "                    A3 = 2 * a * r1 - a\n",
        "                    C3 = 2 * r2\n",
        "                    D_delta = abs(C3 * self.delta_pos[j] - population[i, j])\n",
        "                    X3 = self.delta_pos[j] - A3 * D_delta\n",
        "\n",
        "                    population[i, j] = (X1 + X2 + X3) / 3\n",
        "\n",
        "            print(f\"Iteration {iter+1}/{self.max_iter}, Best Fitness: {self.alpha_score}\")\n",
        "\n",
        "        return self.alpha_pos, self.alpha_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs2UWqGGARGw"
      },
      "outputs": [],
      "source": [
        "def svm_objective(params):\n",
        "    C, gamma = params\n",
        "    C = max(C, 0.1)  # Ensure C is within the valid range\n",
        "    gamma = max(gamma, 0.0001)  # Ensure gamma is within the valid range\n",
        "    svm = SVC(C=C, gamma=gamma)\n",
        "    svm.fit(X_train_tfidf, y_train)\n",
        "    y_pred = svm.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return -accuracy  # Minimize negative accuracy to maximize accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "949CGS1t3F3Y",
        "outputId": "b49553c5-6841-4beb-dcf2-3ab3b5cc0da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 2/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 3/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 4/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 5/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 6/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 7/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 8/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 9/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 10/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 11/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 12/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 13/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 14/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 15/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 16/50, Best Fitness: -0.8777348777348777\n",
            "Iteration 17/50, Best Fitness: -0.8783783783783784\n",
            "Iteration 18/50, Best Fitness: -0.879021879021879\n",
            "Iteration 19/50, Best Fitness: -0.879021879021879\n",
            "Iteration 20/50, Best Fitness: -0.879021879021879\n",
            "Iteration 21/50, Best Fitness: -0.879021879021879\n",
            "Iteration 22/50, Best Fitness: -0.879021879021879\n",
            "Iteration 23/50, Best Fitness: -0.879021879021879\n",
            "Iteration 24/50, Best Fitness: -0.879021879021879\n",
            "Iteration 25/50, Best Fitness: -0.879021879021879\n",
            "Iteration 26/50, Best Fitness: -0.879021879021879\n",
            "Iteration 27/50, Best Fitness: -0.879021879021879\n",
            "Iteration 28/50, Best Fitness: -0.879021879021879\n",
            "Iteration 29/50, Best Fitness: -0.879021879021879\n",
            "Iteration 30/50, Best Fitness: -0.879021879021879\n",
            "Iteration 31/50, Best Fitness: -0.879021879021879\n",
            "Iteration 32/50, Best Fitness: -0.879021879021879\n",
            "Iteration 33/50, Best Fitness: -0.879021879021879\n",
            "Iteration 34/50, Best Fitness: -0.879021879021879\n",
            "Iteration 35/50, Best Fitness: -0.879021879021879\n",
            "Iteration 36/50, Best Fitness: -0.879021879021879\n",
            "Iteration 37/50, Best Fitness: -0.879021879021879\n",
            "Iteration 38/50, Best Fitness: -0.879021879021879\n",
            "Iteration 39/50, Best Fitness: -0.879021879021879\n",
            "Iteration 40/50, Best Fitness: -0.879021879021879\n",
            "Iteration 41/50, Best Fitness: -0.879021879021879\n",
            "Iteration 42/50, Best Fitness: -0.879021879021879\n",
            "Iteration 43/50, Best Fitness: -0.879021879021879\n",
            "Iteration 44/50, Best Fitness: -0.879021879021879\n",
            "Iteration 45/50, Best Fitness: -0.879021879021879\n",
            "Iteration 46/50, Best Fitness: -0.879021879021879\n",
            "Iteration 47/50, Best Fitness: -0.879021879021879\n",
            "Iteration 48/50, Best Fitness: -0.879021879021879\n",
            "Iteration 49/50, Best Fitness: -0.879021879021879\n",
            "Iteration 50/50, Best Fitness: -0.879021879021879\n",
            "Best Parameters: C=2.7251052823959845, gamma=0.9083955668256726\n",
            "Best Accuracy: 0.879021879021879\n"
          ]
        }
      ],
      "source": [
        "# Define parameter bounds (C and gamma)\n",
        "bounds = np.array([[0.1, 100], [0.0001, 1]])\n",
        "\n",
        "population_size = 10\n",
        "max_iter = 50\n",
        "\n",
        "gwo = GreyWolfOptimizer(svm_objective, bounds, population_size, max_iter)\n",
        "best_params, best_fitness = gwo.optimize()\n",
        "\n",
        "print(f\"Best Parameters: C={best_params[0]}, gamma={best_params[1]}\")\n",
        "print(f\"Best Accuracy: {-best_fitness}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hufAovc46rNM"
      },
      "source": [
        "## ANT COLONY OPTIMIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFxtEMFY6txl"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(merge['tweet'], merge['Labels'], test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Scale data (optional but recommended)\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(X_train_tfidf)\n",
        "X_test_scaled = scaler.transform(X_test_tfidf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o7TO_Id63Do"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ACO parameters\n",
        "num_ants = 10  # Number of ants (solutions)\n",
        "num_iterations = 20  # Number of iterations\n",
        "pheromone_decay = 0.5  # Pheromone decay rate\n",
        "alpha = 1.0  # Pheromone weight\n",
        "beta = 2.0  # Heuristic weight\n",
        "\n",
        "# SVM parameter search space\n",
        "C_values = [0.1, 1.0, 10.0]  # List of C values to search\n",
        "gamma_values = ['auto', 'scale']  # List of gamma values to search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHZhqPKw63Ac"
      },
      "outputs": [],
      "source": [
        "# Initialize pheromone trails\n",
        "num_params = len(C_values) * len(gamma_values)\n",
        "pheromones = np.ones((len(C_values), len(gamma_values)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ9YLSmt7Ofr",
        "outputId": "435c35f9-428d-43c8-aa9e-27bf16397126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best SVM Parameters: C = 10.0, gamma = scale\n"
          ]
        }
      ],
      "source": [
        "# ACO loop\n",
        "for iteration in range(num_iterations):\n",
        "    ant_solutions = []\n",
        "\n",
        "    # Construct solutions (parameter sets) for each ant\n",
        "    for ant in range(num_ants):\n",
        "        # Initialize solution set (C index, gamma index)\n",
        "        solution_set = set()\n",
        "\n",
        "        # Build solution for current ant\n",
        "        while len(solution_set) < num_params:\n",
        "            # Calculate probabilities for each parameter combination\n",
        "            probabilities = np.zeros((len(C_values), len(gamma_values)))\n",
        "            total = np.sum(pheromones)\n",
        "\n",
        "            for i in range(len(C_values)):\n",
        "                for j in range(len(gamma_values)):\n",
        "                    if (i, j) not in solution_set:\n",
        "                        probabilities[i, j] = (pheromones[i, j] ** alpha) * (1.0 / total) ** beta\n",
        "\n",
        "            # Select next parameter combination based on probabilities\n",
        "            prob_flat = probabilities.flatten()\n",
        "            chosen_index = np.random.choice(np.arange(len(prob_flat)), p=prob_flat / np.sum(prob_flat))\n",
        "            chosen_params = np.unravel_index(chosen_index, probabilities.shape)\n",
        "\n",
        "            # Add chosen parameter combination to solution set\n",
        "            solution_set.add(chosen_params)\n",
        "\n",
        "        ant_solutions.append(list(solution_set))\n",
        "\n",
        "    # Evaluate solutions and update pheromones\n",
        "    for ant_solution in ant_solutions:\n",
        "        best_accuracy = 0.0\n",
        "        best_params = None\n",
        "\n",
        "        # Evaluate SVM for each parameter combination\n",
        "        for C_idx, gamma_idx in ant_solution:\n",
        "            C = C_values[C_idx]\n",
        "            gamma = gamma_values[gamma_idx]\n",
        "\n",
        "            # Train SVM on current parameters\n",
        "            svm_model = SVC(kernel='rbf', C=C, gamma=gamma, random_state=42)\n",
        "            svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "            # Evaluate SVM model\n",
        "            y_pred = svm_model.predict(X_test_scaled)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            # Update best parameters if accuracy improves\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_params = (C_idx, gamma_idx)\n",
        "\n",
        "        # Update pheromones based on solution quality\n",
        "        pheromones[best_params] += best_accuracy\n",
        "\n",
        "    # Decay pheromones\n",
        "    pheromones *= pheromone_decay\n",
        "\n",
        "# Select best parameters based on highest pheromone levels\n",
        "best_params_idx = np.unravel_index(np.argmax(pheromones), pheromones.shape)\n",
        "best_C = C_values[best_params_idx[0]]\n",
        "best_gamma = gamma_values[best_params_idx[1]]\n",
        "\n",
        "print(f\"Best SVM Parameters: C = {best_C}, gamma = {best_gamma}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAy4uRXR7OY_",
        "outputId": "5b52b476-8824-415f-baa1-48e9aaa42059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final SVM Model Accuracy with Best Parameters: 0.8436\n"
          ]
        }
      ],
      "source": [
        "# Train SVM with best parameters\n",
        "svm_best = SVC(kernel='rbf', C=best_C, gamma=best_gamma, random_state=42)\n",
        "svm_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate SVM model\n",
        "y_pred_best = svm_best.predict(X_test_scaled)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "print(f\"Final SVM Model Accuracy with Best Parameters: {accuracy_best:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQSHp1wRQ4tY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTZAM8UGICEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_r-Q4F6cXCr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29yOH-9DQcWP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHu0NvzUQaFm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEYPpT2jUhK9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6h0nw5EUCtY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcrYuxd3z7jq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze4SySVMy4wk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}